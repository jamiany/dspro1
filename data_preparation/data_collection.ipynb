{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-31T15:47:01.944259Z",
     "start_time": "2024-12-31T15:46:56.668186Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# kaggle urls / names from used datasets\n",
    "urls = [\n",
    "    \"joebeachcapital/30000-spotify-songs\",\n",
    "    \"tomigelo/spotify-audio-features\",\n",
    "    \"imuhammad/audio-features-and-lyrics-of-spotify-songs\",\n",
    "    \"maharshipandya/-spotify-tracks-dataset\",\n",
    "    \"amitanshjoshi/spotify-1million-tracks\",\n",
    "    \"mrmorj/dataset-of-songs-in-spotify\",\n",
    "    \"thedevastator/spotify-tracks-genre-dataset\",\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    # download dataset from kaggle\n",
    "    local_path = kagglehub.dataset_download(url)\n",
    "\n",
    "    files = os.listdir(local_path)\n",
    "\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "    \n",
    "    # read all csv files downloaded from kaggle and collect them in one dataframe\n",
    "    df = None\n",
    "    for csv in csv_files:\n",
    "        full_path = os.path.join(local_path, csv)\n",
    "        \n",
    "        df2 = pd.read_csv(full_path)\n",
    "        if df is None:\n",
    "            df = df2\n",
    "        else:\n",
    "            df = pd.concat([df, df2], axis=0)\n",
    "\n",
    "    # store the concatenated dataframe\n",
    "    dataframes.append(df)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tamino/Projects/hslu/dspro/dspro1/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j8/6g668vcj4tg5h1dhl577p6yw0000gn/T/ipykernel_1135/3525510524.py:32: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(full_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "72d84d243e1cccec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T15:47:01.948761Z",
     "start_time": "2024-12-31T15:47:01.945394Z"
    }
   },
   "source": [
    "# column names of our target dataset\n",
    "columns = ['id', 'name', 'artist', 'album', 'release_date', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'valence']\n",
    "\n",
    "# the column names of the datasets in the same order as the target column names to allow easy mapping\n",
    "dataset_column_names = [\n",
    "    ['track_id', 'track_name', 'track_artist', 'track_album_name', 'track_album_release_date', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'valence'],\n",
    "    ['track_id', 'track_name', 'artist_name', None, None, 'acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'valence'],\n",
    "    ['track_id', 'track_name', 'track_artist', 'track_album_name', 'track_album_release_date', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'valence'],\n",
    "    ['track_id', 'track_name', 'artists', 'album_name', None, 'acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'valence'],\n",
    "    ['track_id', 'track_name', 'artist_name', None, 'year', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'valence'],\n",
    "    ['id', 'song_name', None, None, None, 'acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'valence'],\n",
    "    ['track_id', 'track_name', 'artists', 'album_name', None, 'acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'valence'],\n",
    "]\n",
    "\n",
    "# prioritize datasets with more columns available\n",
    "order = [0, 2, 3, 6, 4, 1, 5]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1fe02647180166e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T15:47:01.951729Z",
     "start_time": "2024-12-31T15:47:01.949498Z"
    }
   },
   "source": [
    "# transform the dataset_column_names to a mapping dict that pandas can use to rename all columns\n",
    "column_mappings = []\n",
    "\n",
    "for column_names in dataset_column_names:\n",
    "    mapping = {}\n",
    "\n",
    "    for idx, column_name in enumerate(column_names):\n",
    "        if column_name is None:\n",
    "            continue\n",
    "\n",
    "        mapping[column_name] = columns[idx]\n",
    "    \n",
    "    column_mappings.append(mapping)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5eafe489bd6ae723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T15:47:02.788338Z",
     "start_time": "2024-12-31T15:47:01.953059Z"
    }
   },
   "source": [
    "# initialize an empty dataframe to collect all the data from the different sets\n",
    "df_complete = pd.DataFrame(data=[], columns=columns)\n",
    "id_cache = []\n",
    "\n",
    "# collect the data from each dataset in the specified order\n",
    "for idx in order:\n",
    "    print(f'processing dataset {idx}')\n",
    "\n",
    "    df = dataframes[idx]\n",
    "    mapping = column_mappings[idx]\n",
    "    \n",
    "    # rename the columns to the shared target name\n",
    "    df_temp = df.rename(columns=mapping)\n",
    "    mapped_columns = list(mapping.values())\n",
    "    \n",
    "    # remove all additional columns that the dataset may contain\n",
    "    df_subset = df_temp[mapped_columns]\n",
    "    \n",
    "    # remove all duplicates from the dataset\n",
    "    df_subset = df_subset.drop_duplicates(subset=['id'])\n",
    "    \n",
    "    # remove all rows which were already provided by an earlier dataset\n",
    "    df_cleaned = df_subset[~df_subset['id'].isin(id_cache)]\n",
    "\n",
    "    # skip datasets with no new songs remaining\n",
    "    if len(df_cleaned) == 0:\n",
    "        print('no new song data')\n",
    "        continue\n",
    "\n",
    "    # concatenate the new dataset to the complete dataset\n",
    "    id_cache.extend(df_cleaned['id'].tolist())\n",
    "    if not df_complete.empty:\n",
    "        df_complete = pd.concat([df_complete, df_cleaned], ignore_index=True)\n",
    "    else:\n",
    "        df_complete = df_cleaned\n",
    "    \n",
    "    print('appended song data')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset 0\n",
      "appended song data\n",
      "processing dataset 2\n",
      "no new song data\n",
      "processing dataset 3\n",
      "appended song data\n",
      "processing dataset 6\n",
      "no new song data\n",
      "processing dataset 4\n",
      "appended song data\n",
      "processing dataset 1\n",
      "appended song data\n",
      "processing dataset 5\n",
      "appended song data\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T15:47:03.265753Z",
     "start_time": "2024-12-31T15:47:02.789102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert release date to release year\n",
    "def extract_year(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    elif '-' in str(value):\n",
    "        return float(str(value).split('-')[0])\n",
    "    else:\n",
    "        return float(value)\n",
    "\n",
    "df_complete['release_year'] = df_complete['release_date'].apply(extract_year)\n",
    "df_complete.drop(columns=['release_date'], inplace=True)"
   ],
   "id": "ca6e5732b85bdd10",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "b8b6b9bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T15:47:03.279486Z",
     "start_time": "2024-12-31T15:47:03.266585Z"
    }
   },
   "source": [
    "# normalize non normalized columns\n",
    "features = ['loudness', 'tempo']\n",
    "scaler = MinMaxScaler()\n",
    "df_complete[features] = scaler.fit_transform(df_complete[features])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "687f5fa142102aa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T15:47:08.935949Z",
     "start_time": "2024-12-31T15:47:03.280171Z"
    }
   },
   "source": [
    "# store the complete dataset locally so that the data collection process only needs to run once\n",
    "df_complete.to_csv(path_or_buf='data.csv')"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T15:47:08.938105Z",
     "start_time": "2024-12-31T15:47:08.936648Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7e942a85350a33a1",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
